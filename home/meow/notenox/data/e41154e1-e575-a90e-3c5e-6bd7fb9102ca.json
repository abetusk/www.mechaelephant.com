{
  "id": "e41154e1-e575-a90e-3c5e-6bd7fb9102ca",
  "timestamp": 1729373920.437,
  "keyword": [
    "title:math",
    "math",
    "mathematics",
    "statistics",
    "ai",
    "ann",
    "nn"
  ],
  "extra": [],
  "note": "Talks about the confusion caused from stastical insight from the 'bias vs. variance' tradeoff. Statiticians are taught about a bias variance tradeoff where high confidence in bias leads to large variance and high confidence in (small) variance leads to low confidence in bias (large bias). For KNN, K large leads to high variance but low bias. K small leads to low variance but high bias. So there's a kind of 'overfitting' intuition where you have to sacrifice one for the other.\\nNN seem to subvert this insight by providing both low variance and low bias. The move from having all data to using only a sub-set of data for training with an eye towards generalization means that the problem statement is different so generalization breaks down.",
  "link": [
    "https://x.com/AliciaCurth/status/1841817856142348529",
    "https://github.com/abetusk/papers/blob/release/MathMisc/classical-statistics-intuitions-dont-generalize-well_curth.pdf"
  ]
}